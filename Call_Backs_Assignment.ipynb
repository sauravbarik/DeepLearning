{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "6eIZ--YW1-xT",
    "outputId": "c71ab354-f4a1-44b2-dbc9-1d46d5bb7888"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mMzNW-ci4ckZ",
    "outputId": "ff32e70d-b9b7-4123-b0cf-138d0cc97a57"
   },
   "outputs": [],
   "source": [
    "# !ls '/content/gdrive/My Drive/Colab Notebooks/Callbacks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S5W4wmwH10MB",
    "outputId": "4aea78b0-6059-4d6d-8fbe-a6115a0e7dab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "import random as rn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.python.keras import backend as K\n",
    "import datetime\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TQUx1lzaxHR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t_4szKaE6B1J",
    "outputId": "b9d153c3-f06f-447a-fd61-99017665b3ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2) (20000,)\n"
     ]
    }
   ],
   "source": [
    "Y = data['label']\n",
    "X = data.drop(['label'],axis=1)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "86kM3c1gakCS",
    "outputId": "3eb13637-07ed-4ad6-b426-394ceec48694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 2)\n",
      "(14000, 2)\n",
      "(6000, 2)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2) \n",
    "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4M9Bb2Ws5SH3"
   },
   "outputs": [],
   "source": [
    "class MyClass(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,validation_data):\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, F1 score, auc_score]\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': [],'F1_score':[],'auc_score':[]}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))\n",
    "\n",
    "        val_predict = (np.asarray(self.model.predict(self.x_val))).round()\n",
    "        val_targ = self.y_val\n",
    "        f1 = round(f1_score(val_targ, val_predict,average='micro'),4)\n",
    "        auc = round(roc_auc_score(val_targ,val_predict),4)\n",
    "        print(' - f1 score - {} - auc_score - {}'.format(f1,auc))\n",
    "        self.history['F1_score'].append(logs.get('F1_score'))\n",
    "        self.history['auc_score'].append(logs.get('auc_score'))\n",
    "\n",
    "        \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gQi0dtp5VEz"
   },
   "source": [
    "<h1>Model  1</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_yGRwdzaiz4"
   },
   "outputs": [],
   "source": [
    "class LrReducer(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.wait = 0\n",
    "        self.best_score = -1.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_score = logs.get('val_acc')\n",
    "        self.wait = 0\n",
    "        # lr = self.get_value(self.model.optimizer.lr)\n",
    "        lr  = K.get_value(self.model.optimizer.lr)\n",
    "        print('Current Learning rate is ',lr)\n",
    "        if current_score is not None and current_score > self.best_score:\n",
    "            self.best_score = current_score\n",
    "            K.set_value(self.model.optimizer.lr, (lr - (lr*0.1)))\n",
    "        else:\n",
    "            if self.wait % 3 == 0:\n",
    "                K.set_value(self.model.optimizer.lr, (lr - (lr*0.05)))\n",
    "        self.wait += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMG6FsgAhaLI"
   },
   "outputs": [],
   "source": [
    "class TerminateNaN(tf.keras.callbacks.Callback):\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "2K9_kbX_ZV1O",
    "outputId": "3b9e500f-86c3-4cd9-a039-0ab38f9731b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "13860/14000 [============================>.] - ETA: 0s - loss: 0.6971 - accuracy: 0.4987 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49683, saving model to model_save/weights-01-0.4968.hdf5\n",
      "Current Learning rate is  0.01\n",
      "14000/14000 [==============================] - 9s 656us/sample - loss: 0.6971 - accuracy: 0.4986 - val_loss: 0.6961 - val_accuracy: 0.4968\n",
      "Epoch 2/10\n",
      "13890/14000 [============================>.] - ETA: 0s - loss: 0.6956 - accuracy: 0.5001 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.49683\n",
      "Current Learning rate is  0.0095\n",
      "14000/14000 [==============================] - 6s 455us/sample - loss: 0.6955 - accuracy: 0.5005 - val_loss: 0.6948 - val_accuracy: 0.4968\n",
      "Epoch 3/10\n",
      "13890/14000 [============================>.] - ETA: 0s - loss: 0.6957 - accuracy: 0.4962 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.49683\n",
      "Current Learning rate is  0.009025\n",
      "14000/14000 [==============================] - 6s 449us/sample - loss: 0.6957 - accuracy: 0.4962 - val_loss: 0.6933 - val_accuracy: 0.4968\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7effd4710750>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "\n",
    "#Dense hidden layer 1\n",
    "layer1 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform( minval=0.,maxval = 1.,seed=30))(input_layer)\n",
    "\n",
    "#Dense hidden layer 2\n",
    "layer2 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=42))(layer1)\n",
    "\n",
    "#Dense hidden layer 3\n",
    "layer3 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=22))(layer2)\n",
    "\n",
    "#Dense hidden layer 4\n",
    "layer4 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=32))(layer3)\n",
    "\n",
    "#Dense hidden layer 5\n",
    "layer5 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=12))(layer4)\n",
    "\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.RandomUniform(minval = 0.,maxval = 1.,seed=0))(layer5)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "\n",
    "#Callbacks\n",
    "history_own = MyClass(validation_data=(X_test,y_test)) \n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=2, verbose=1)\n",
    "lrschedule = LrReducer()\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max')\n",
    "terminate_nan = TerminateNaN()\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10, validation_data=(X_test,y_test), batch_size=30, callbacks=[history_own,earlystop,checkpoint,terminate_nan,lrschedule,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 11529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cd6fa559dbc526f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cd6fa559dbc526f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "13860/14000 [============================>.] - ETA: 0s - loss: 2076603827.9707 - accuracy: 0.4981 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49683, saving model to model_save/weights-01-0.4968.hdf5\n",
      "Current Learning rate is  0.01\n",
      "14000/14000 [==============================] - 9s 677us/sample - loss: 2055837789.6979 - accuracy: 0.4979 - val_loss: 0.6933 - val_accuracy: 0.4968\n",
      "Epoch 2/10\n",
      "13950/14000 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4993 - f1 score - 0.5032 - auc_score - 0.5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.49683 to 0.50317, saving model to model_save/weights-02-0.5032.hdf5\n",
      "Current Learning rate is  0.0095\n",
      "14000/14000 [==============================] - 7s 533us/sample - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6931 - val_accuracy: 0.5032\n",
      "Epoch 3/10\n",
      "13950/14000 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5050 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.50317\n",
      "Current Learning rate is  0.009025\n",
      "14000/14000 [==============================] - 7s 480us/sample - loss: 0.6932 - accuracy: 0.5046 - val_loss: 0.6933 - val_accuracy: 0.4968\n",
      "Epoch 4/10\n",
      "13890/14000 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4999 - f1 score - 0.4968 - auc_score - 0.5\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.50317\n",
      "Current Learning rate is  0.00857375\n",
      "14000/14000 [==============================] - 6s 462us/sample - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4968\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa24e600c50>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "\n",
    "#Dense hidden layer 1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval = 0.,maxval=1.,seed=30))(input_layer)\n",
    "\n",
    "#Dense hidden layer 2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=42))(layer1)\n",
    "\n",
    "#Dense hidden layer 3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=22))(layer2)\n",
    "\n",
    "#Dense hidden layer 4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=32))(layer3)\n",
    "\n",
    "#Dense hidden layer 5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval = 1.,seed=12))(layer4)\n",
    "\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.RandomUniform(minval = 0.,maxval = 1.,seed=0))(layer5)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "\n",
    "#Callbacks\n",
    "history_own = MyClass(validation_data=(X_test,y_test)) \n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=2, verbose=1)\n",
    "lrschedule = LrReducer()\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max')\n",
    "terminate_nan = TerminateNaN()\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10, validation_data=(X_test,y_test), batch_size=30, callbacks=[history_own,earlystop,checkpoint,terminate_nan,lrschedule,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 13018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-48ca568191045d0b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-48ca568191045d0b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 120 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.14 in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/saurav/anaconda3/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "13950/14000 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.5933 - f1 score - 0.6505 - auc_score - 0.651\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.65050, saving model to model_save/weights-01-0.6505.hdf5\n",
      "Current Learning rate is  0.01\n",
      "14000/14000 [==============================] - 10s 694us/sample - loss: 0.6643 - accuracy: 0.5939 - val_loss: 0.6364 - val_accuracy: 0.6505\n",
      "Epoch 2/10\n",
      "13980/14000 [============================>.] - ETA: 0s - loss: 0.6196 - accuracy: 0.6574 - f1 score - 0.6405 - auc_score - 0.6394\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.65050\n",
      "Current Learning rate is  0.0095\n",
      "14000/14000 [==============================] - 7s 523us/sample - loss: 0.6194 - accuracy: 0.6576 - val_loss: 0.6406 - val_accuracy: 0.6405\n",
      "Epoch 3/10\n",
      "13890/14000 [============================>.] - ETA: 0s - loss: 0.6093 - accuracy: 0.6643 - f1 score - 0.6582 - auc_score - 0.6588\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.65050 to 0.65817, saving model to model_save/weights-03-0.6582.hdf5\n",
      "Current Learning rate is  0.009025\n",
      "14000/14000 [==============================] - 7s 501us/sample - loss: 0.6087 - accuracy: 0.6646 - val_loss: 0.6197 - val_accuracy: 0.6582\n",
      "Epoch 4/10\n",
      "13920/14000 [============================>.] - ETA: 0s - loss: 0.6050 - accuracy: 0.6671 - f1 score - 0.6567 - auc_score - 0.6574\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.65817\n",
      "Current Learning rate is  0.00857375\n",
      "14000/14000 [==============================] - 7s 475us/sample - loss: 0.6047 - accuracy: 0.6673 - val_loss: 0.6218 - val_accuracy: 0.6567\n",
      "Epoch 5/10\n",
      "13770/14000 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.6684 - f1 score - 0.6513 - auc_score - 0.6504\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.65817\n",
      "Current Learning rate is  0.008145062\n",
      "14000/14000 [==============================] - 5s 324us/sample - loss: 0.6002 - accuracy: 0.6690 - val_loss: 0.6287 - val_accuracy: 0.6513\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2295ae190>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "\n",
    "#Dense hidden layer 1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=30))(input_layer)\n",
    "\n",
    "#Dense hidden layer 2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=42))(layer1)\n",
    "\n",
    "#Dense hidden layer 3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=22))(layer2)\n",
    "\n",
    "#Dense hidden layer 4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=32))(layer3)\n",
    "\n",
    "#Dense hidden layer 5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=12))(layer4)\n",
    "\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(layer5)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "\n",
    "#Callbacks\n",
    "history_own = MyClass(validation_data=(X_test,y_test)) \n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=2, verbose=1)\n",
    "lrschedule = LrReducer()\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max')\n",
    "terminate_nan = TerminateNaN()\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=10, write_graph=True,write_grads=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10, validation_data=(X_test,y_test), batch_size=30, callbacks=[history_own,earlystop,checkpoint,terminate_nan,lrschedule,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 10595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aae2add80ec71fd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aae2add80ec71fd3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "13890/14000 [============================>.] - ETA: 0s - loss: 0.6580 - accuracy: 0.6131 - f1 score - 0.6485 - auc_score - 0.648\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64850, saving model to model_save/weights-01-0.6485.hdf5\n",
      "Current Learning rate is  0.01\n",
      "14000/14000 [==============================] - 9s 644us/sample - loss: 0.6577 - accuracy: 0.6136 - val_loss: 0.6291 - val_accuracy: 0.6485\n",
      "Epoch 2/10\n",
      "13950/14000 [============================>.] - ETA: 0s - loss: 0.6125 - accuracy: 0.6636 - f1 score - 0.6523 - auc_score - 0.6515\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.64850 to 0.65233, saving model to model_save/weights-02-0.6523.hdf5\n",
      "Current Learning rate is  0.0095\n",
      "14000/14000 [==============================] - 8s 576us/sample - loss: 0.6123 - accuracy: 0.6639 - val_loss: 0.6283 - val_accuracy: 0.6523\n",
      "Epoch 3/10\n",
      "13980/14000 [============================>.] - ETA: 0s - loss: 0.6035 - accuracy: 0.6692 - f1 score - 0.668 - auc_score - 0.6681\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.65233 to 0.66800, saving model to model_save/weights-03-0.6680.hdf5\n",
      "Current Learning rate is  0.009025\n",
      "14000/14000 [==============================] - 7s 508us/sample - loss: 0.6035 - accuracy: 0.6694 - val_loss: 0.6124 - val_accuracy: 0.6680\n",
      "Epoch 4/10\n",
      "13920/14000 [============================>.] - ETA: 0s - loss: 0.5991 - accuracy: 0.6743 - f1 score - 0.6622 - auc_score - 0.6623\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.66800\n",
      "Current Learning rate is  0.00857375\n",
      "14000/14000 [==============================] - 7s 486us/sample - loss: 0.5989 - accuracy: 0.6744 - val_loss: 0.6127 - val_accuracy: 0.6622\n",
      "Epoch 5/10\n",
      "13920/14000 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.6731 - f1 score - 0.6653 - auc_score - 0.6655\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.66800\n",
      "Current Learning rate is  0.008145062\n",
      "14000/14000 [==============================] - 7s 492us/sample - loss: 0.5973 - accuracy: 0.6731 - val_loss: 0.6130 - val_accuracy: 0.6653\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa22f67d550>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "\n",
    "#Dense hidden layer 1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=30))(input_layer)\n",
    "\n",
    "#Dense hidden layer 2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=42))(layer1)\n",
    "\n",
    "#Dense hidden layer 3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=22))(layer2)\n",
    "\n",
    "#Dense hidden layer 4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=32))(layer3)\n",
    "\n",
    "#Dense hidden layer 5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.he_uniform(seed=12))(layer4)\n",
    "\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.he_uniform(seed=0))(layer5)\n",
    "\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "\n",
    "#Callbacks\n",
    "history_own = MyClass(validation_data=(X_test,y_test)) \n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=2, verbose=1)\n",
    "lrschedule = LrReducer()\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max')\n",
    "terminate_nan = TerminateNaN()\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adagrad(0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=10, validation_data=(X_test,y_test), batch_size=30, callbacks=[history_own,earlystop,checkpoint,terminate_nan,lrschedule,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 14240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7a00722c7ad1843a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7a00722c7ad1843a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
